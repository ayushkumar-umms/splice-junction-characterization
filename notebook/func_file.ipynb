{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions File (func_file.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following file is the file that contains the functions for extracting the location of each splice site of a splice junction, determines the phyloP score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import subprocess\n",
    "from collections import Counter\n",
    "from sys import argv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Splice Junction Info Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs and Intital setup of function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extract_spljnc function takes in a splice junction, gtf, strand, constitutive exons, and gtf_exons.\n",
    "Constitutive exons are all the exons that are found at that specific chromosome and strand of the splice junction.\n",
    "GTF_exons is all the exons on that strand and chromsome. \n",
    "These two are important for identifying the closest exons and constitutive exons to each splice site\n",
    "\n",
    "Steps of function:\n",
    "1. Based on strand determine which is end is 5' and 3'\n",
    "2. Search each end though the GTF and extract all relevant features\n",
    "3. See what features are found at that site (gene, exon, CDS ..)\n",
    "4. Store information in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spljnc(splice_jnc, gtf, strand, cons_exons, gtf_exons):\n",
    "    #Define strand and location of 5' and 3' ends\n",
    "    if  strand == '+':\n",
    "        five_ss = splice_jnc.first_base\n",
    "        three_ss = splice_jnc.last_base\n",
    "    elif strand == '-':\n",
    "        five_ss = splice_jnc.last_base\n",
    "        three_ss = splice_jnc.first_base\n",
    "    # Extract the parts of GTF file that contain the 5' end\n",
    "    extract_five_info = (gtf['start'] <= five_ss) & (gtf['end'] >= five_ss)\n",
    "    five_info_df = gtf[extract_five_info]\n",
    "    # What are the features of the strand that are found in this junction (gene,transcript, exon)?\n",
    "    five_features = set(five_info_df['feature'].tolist())\n",
    "    possible_elements = set(['gene','transcript','exon','CDS','start_codon','stop_codon','five_prime_utr','three_prime_utr'])\n",
    "    # Create a dictionary for the features of the 5' end and set it to 1 or 0\n",
    "    dict_five_sam = {ft:1 for ft in possible_elements.intersection(five_features)}\n",
    "    dict_five_dif = {ft:0 for ft in possible_elements.difference(five_features)}\n",
    "    dict_five = {**dict_five_dif, **dict_five_sam}\n",
    "    dict_five = {key.replace('_',''): val for key, val in dict_five.items()}\n",
    "    # Extract the parts of GTF file that contain the 3' end (look at 5' comments for role of each part of code)\n",
    "    extract_three_info = (gtf['start'] <= three_ss) & (gtf['end'] >= three_ss)\n",
    "    three_info_df = gtf[extract_three_info]\n",
    "    three_features = set(three_info_df['feature'].tolist())\n",
    "    dict_three_sam = {ft:1 for ft in possible_elements.intersection(three_features)}\n",
    "    dict_three_dif = {ft:0 for ft in possible_elements.difference(three_features)}\n",
    "    dict_three = {**dict_three_dif, **dict_three_sam}\n",
    "    dict_three = {key.replace('_',''): val for key, val in dict_three.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intron is not a defined feature in the GTF file, so we need to define intron as anything that is contained within a transcript and not an exon by looking at the transcript ID column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'five_info_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-88cb6b6a7048>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfive_info_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'feature'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'transcript'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfive_info_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'feature'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'transcript'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mfive_info_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'feature'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exon'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mfive_intron\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mfive_intron\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'five_info_df' is not defined"
     ]
    }
   ],
   "source": [
    "    if (five_info_df['feature'].tolist().count('transcript')!= 0):\n",
    "        if (five_info_df['feature'].tolist().count('transcript')==five_info_df['feature'].tolist().count('exon')):\n",
    "            five_intron= 0\n",
    "        else:\n",
    "            five_intron = 1\n",
    "    else:\n",
    "        five_intron = np.nan\n",
    "    if (three_info_df['feature'].tolist().count('transcript')!= 0):\n",
    "        if (three_info_df['feature'].tolist().count('transcript')==three_info_df['feature'].tolist().count('exon')):\n",
    "            three_intron= 0\n",
    "        else:\n",
    "            three_intron = 1\n",
    "    else:\n",
    "        three_intron = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining if constitutive vs alternative intron/exon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting the information where the splice site lies, we needed to determine if the exon/intron was constitutive.\n",
    "\n",
    "1. Took all the genes that each splice site lies in\n",
    "2. For each gene, identified whether the splice site was only in exons, only in introns, or a mix of both\n",
    "3. If only in exon/only in intron, then I looked to see if it was in the constitutive exon list. \n",
    "4. A constitutive intron is contained between two constitutive exons. Therefore, I looked to see if the two closest exons were constitutive or not \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking if it is in a constiutive or alternative exon/intron\n",
    "    five_con_intron =[] # constitutive intron 5'\n",
    "    five_con_exon = [] # constitutive exon 5'\n",
    "    genes_three = three_info_df['gene_id'].unique().tolist() #genes on 3' end\n",
    "    genes_five = five_info_df['gene_id'].unique().tolist() # genes on 5' end\n",
    "    junctions_five = [] # name of region where 5' end is located when 5' end and 3' end share the same gene location\n",
    "    #other_junctions_five = [] #name of region where 5' end is located when 5' end and 3' end DON'T share the same gene location\n",
    "    #intersecting_genes = list(set(genes_five).intersection(genes_three)) # which genes are 5' end and 3' end found\n",
    "    #start = time.process_time()\n",
    "    for geneid in genes_five:\n",
    "        five_info_df_gene = five_info_df[five_info_df['gene_id'].isin([geneid])]\n",
    "        exonids_five = five_info_df_gene[five_info_df_gene['feature'].isin(['exon'])]['exon_id'].tolist()\n",
    "\n",
    "        if five_info_df_gene[five_info_df_gene['feature'].isin(['transcript'])].empty: # if no transcripts then can't be constituve exon or intron\n",
    "            five_con_intron.append(np.nan)\n",
    "            five_con_exon.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        elif (five_info_df_gene['feature'].tolist().count('transcript')==five_info_df_gene['feature'].tolist().count('exon')): # if number of exons found = number of transcripts found, we check the      constitutive exon dataframe if that exon and corresponding geneid are present\n",
    "            exon_type = cons_exons[cons_exons['exon_id'].isin(exonids_five) & cons_exons['gene_id'].isin([geneid])]\n",
    "            if not exon_type.empty:\n",
    "                five_con_exon.append(geneid + ':exon:' + str(exon_type.iloc[0]['start']) +'-'+ str(exon_type.iloc[0]['end']))\n",
    "            else:\n",
    "                five_con_exon.append(0)\n",
    "            #print(time.process_time() - st,1)\n",
    "            five_con_intron.append(np.nan)\n",
    "            listofexons = five_info_df_gene[five_info_df_gene['feature'].isin(['exon'])]\n",
    "            listofexons2 = listofexons.copy()\n",
    "            listofexons2['string'] = geneid + ':chr' + listofexons['seqname'] +':exon:' + listofexons['start'].astype(str) + '-' + listofexons['end'].astype(str)\n",
    "            junctions_five.extend(listofexons2['string'].tolist())\n",
    "        elif (five_info_df_gene['feature'].tolist().count('transcript')!=five_info_df_gene['feature'].tolist().count('exon')) and (five_info_df_gene['feature'].tolist().count('exon') > 0):\n",
    "            # if number of trasncripts differs from number of exons then we need to look at each exon and intron\n",
    "            five_con_exon.append(0)\n",
    "            five_con_intron.append(0)\n",
    "            listofexons = five_info_df_gene[five_info_df_gene['feature'].isin(['exon'])]\n",
    "            listofexons2 = listofexons.copy()\n",
    "            listofexons2['string'] = geneid + ':chr' + listofexons['seqname'] +':exon:' + listofexons['start'].astype(str) + '-' + listofexons['end'].astype(str)\n",
    "            junctions_five.extend(listofexons2['string'].tolist())\n",
    "            cnt_transcripts = Counter(five_info_df_gene['transcript_id'].dropna().tolist())\n",
    "            sing_trans =  [a for a, b in cnt_transcripts.items() if b == 1]\n",
    "            #count number of transcripts that are only repeated once in five_info_df_gene then it is an intron\n",
    "            for tran in sing_trans:\n",
    "                nearbyexons = gtf_exons[gtf_exons['transcript_id'].isin([tran])]\n",
    "                #print(time.process_time() - st,'c')\n",
    "                five_near_end_exon = nearbyexons[nearbyexons['end'] < five_ss].end.max()\n",
    "                five_near_start_exon = nearbyexons[nearbyexons['start'] > five_ss].start.min()\n",
    "                junctions_five.append(str(geneid + ':chr'+ splice_jnc.chr + ':intron:'+ str(five_near_end_exon + 1) +'-' + str(five_near_start_exon-1)))\n",
    "        else: #Only transcripts and no exons\n",
    "             # Look at closest exons left and right of the coordinate\n",
    "            nearbyexons = gtf_exons[gtf_exons['gene_id'].isin([geneid])]\n",
    "            five_near_end_exon = (nearbyexons['end']==(nearbyexons[nearbyexons['end'] < five_ss].end.max()))\n",
    "            five_end = nearbyexons[five_near_end_exon]['exon_id'].tolist()\n",
    "            five_near_start_exon = (nearbyexons['start'] == (nearbyexons[nearbyexons['start'] > five_ss].start.min()))\n",
    "            five_start = nearbyexons[five_near_start_exon]['exon_id'].tolist()\n",
    "            #five_start = nearbyexons[nearbyexons['start'] == (nearbyexons[nearbyexons['start'] > five_ss].start.min())]['exon_id'].unique.tolist()\n",
    "            # if exons nearby are both constitutive exons then it must be in an constitutive intron\n",
    "            five_start_df = cons_exons[cons_exons['exon_id'].isin(five_start) & cons_exons['gene_id'].isin([geneid])]\n",
    "            five_end_df = cons_exons[cons_exons['exon_id'].isin(five_end) & cons_exons['gene_id'].isin([geneid])]\n",
    "            if cons_exons[cons_exons['exon_id'].isin(five_start) & cons_exons['gene_id'].isin([geneid])].empty or cons_exons[cons_exons['exon_id'].isin(five_end) & cons_exons['gene_id'].isin([geneid])]. empty:\n",
    "                five_con_intron.append(0)\n",
    "            else:\n",
    "                five_con_intron.append(geneid + ':intron:' + str(five_end_df.iloc[0]['end'] +1) +'-'+  str(five_start_df.iloc[0]['start']-1))\n",
    "            five_con_exon.append(np.nan)\n",
    "            cnt_transcripts = Counter(five_info_df_gene['transcript_id'].dropna().tolist())\n",
    "            sing_trans =  [a for a, b in cnt_transcripts.items() if b == 1]\n",
    "            for tran in sing_trans:\n",
    "                nearbyexons = gtf_exons[gtf_exons['transcript_id'].isin([tran])]\n",
    "                five_near_end_exon = nearbyexons[nearbyexons['end'] < five_ss].end.max()\n",
    "                five_near_start_exon = nearbyexons[nearbyexons['start'] > five_ss].start.min()\n",
    "                junctions_five.append(str(geneid + ':chr'+ splice_jnc.chr + ':intron:'+ str(five_near_end_exon+1) +'-' + str(five_near_start_exon-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same process is repeated for the 3' splice site below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    three_con_intron =[]\n",
    "    three_con_exon = []\n",
    "    junctions_three = []\n",
    "    #other_junctions_three = []\n",
    "    for geneid in genes_three:\n",
    "        three_info_df_gene = three_info_df[three_info_df['gene_id'].isin([geneid])]\n",
    "        exonids_three = three_info_df_gene[three_info_df_gene['feature'].isin(['exon'])]['exon_id'].tolist()\n",
    "        if three_info_df_gene[three_info_df_gene['feature'].isin(['transcript'])].empty:\n",
    "            three_con_intron.append(np.nan)\n",
    "            three_con_exon.append(np.nan)\n",
    "            continue\n",
    "        elif (three_info_df_gene['feature'].tolist().count('transcript')==three_info_df_gene['feature'].tolist().count('exon')):\n",
    "            #st = time.process_time()\n",
    "            exon_type = cons_exons[cons_exons['exon_id'].isin(exonids_three) & cons_exons['gene_id'].isin([geneid])]\n",
    "            if not exon_type.empty:\n",
    "                three_con_exon.append(geneid + ':exon:' + str(exon_type.iloc[0]['start']) +'-'+ str(exon_type.iloc[0]['end']))\n",
    "            else:\n",
    "                three_con_exon.append(0)\n",
    "            #print(time.process_time() - st,1)\n",
    "            three_con_intron.append(np.nan)\n",
    "            listofexons = three_info_df_gene[three_info_df_gene['feature'].isin(['exon'])]\n",
    "            listofexons2 = listofexons.copy()\n",
    "            listofexons2['string'] = geneid + ':chr' + listofexons['seqname'] +':exon:' + listofexons['start'].astype(str) + '-' + listofexons['end'].astype(str)\n",
    "            junctions_three.extend(listofexons2['string'].tolist())\n",
    "        elif (three_info_df_gene['feature'].tolist().count('transcript')!=three_info_df_gene['feature'].tolist().count('exon')) and (three_info_df_gene['feature'].tolist().count('exon') > 0):\n",
    "            three_con_exon.append(0)\n",
    "            three_con_intron.append(0)\n",
    "            listofexons = three_info_df_gene[three_info_df_gene['feature'].isin(['exon'])]\n",
    "            listofexons2 = listofexons.copy()\n",
    "            listofexons2['string'] = geneid + ':chr' + listofexons['seqname'] +':exon:' + listofexons['start'].astype(str) + '-' + listofexons['end'].astype(str)\n",
    "        elif (three_info_df_gene['feature'].tolist().count('transcript')!=three_info_df_gene['feature'].tolist().count('exon')) and (three_info_df_gene['feature'].tolist().count('exon') > 0):\n",
    "            three_con_exon.append(0)\n",
    "            three_con_intron.append(0)\n",
    "            listofexons = three_info_df_gene[three_info_df_gene['feature'].isin(['exon'])]\n",
    "            listofexons2 = listofexons.copy()\n",
    "            listofexons2['string'] = geneid + ':chr' + listofexons['seqname'] +':exon:' + listofexons['start'].astype(str) + '-' + listofexons['end'].astype(str)\n",
    "            junctions_three.extend(listofexons2['string'].tolist())\n",
    "            cnt_transcripts = Counter(three_info_df_gene['transcript_id'].dropna().tolist())\n",
    "            sing_trans =  [a for a, b in cnt_transcripts.items() if b == 1]\n",
    "            for tran in sing_trans:\n",
    "                nearbyexons = gtf_exons[gtf_exons['transcript_id'].isin([tran])]\n",
    "                three_near_end_exon = nearbyexons[nearbyexons['end'] < three_ss].end.max()\n",
    "                three_near_start_exon = nearbyexons[nearbyexons['start'] > three_ss].start.min()\n",
    "                junctions_three.append(str(geneid + ':chr'+ splice_jnc.chr + ':intron:'+ str(three_near_end_exon + 1) +'-' + str(three_near_start_exon-1)))\n",
    "        else:\n",
    "            nearbyexons = gtf_exons[gtf_exons['gene_id'].isin([geneid])]\n",
    "            three_near_end_exon = (nearbyexons['end']==(nearbyexons[nearbyexons['end'] < three_ss].end.max()))\n",
    "            three_end = nearbyexons[three_near_end_exon]['exon_id'].tolist()\n",
    "            three_near_start_exon = (nearbyexons['start'] == (nearbyexons[nearbyexons['start'] > three_ss].start.min()))\n",
    "            three_start = nearbyexons[three_near_start_exon]['exon_id'].tolist()\n",
    "            three_start_df = cons_exons[cons_exons['exon_id'].isin(three_start)& cons_exons['gene_id'].isin([geneid])]\n",
    "            three_end_df = cons_exons[cons_exons['exon_id'].isin(three_end) & cons_exons['gene_id'].isin([geneid])]\n",
    "            if cons_exons[cons_exons['exon_id'].isin(three_start)& cons_exons['gene_id'].isin([geneid])].empty or cons_exons[cons_exons['exon_id'].isin(three_end) & cons_exons['gene_id'].                isin([geneid])].empty:\n",
    "                three_con_intron.append(0)\n",
    "            else:\n",
    "                three_con_intron.append(geneid + ':intron:' + str(three_end_df.iloc[0]['end'] +1 ) +'-'+  str(three_start_df.iloc[0]['start']-1))\n",
    "            three_con_exon.append(np.nan)\n",
    "            #print(time.process_time() - st,3)\n",
    "            cnt_transcripts = Counter(three_info_df_gene['transcript_id'].dropna().tolist())\n",
    "            sing_trans =  [a for a, b in cnt_transcripts.items() if b == 1]\n",
    "            for tran in sing_trans:\n",
    "                nearbyexons = gtf_exons[gtf_exons['transcript_id'].isin([tran])]\n",
    "                three_near_end_exon = nearbyexons[nearbyexons['end'] < three_ss].end.max()\n",
    "                three_near_start_exon = nearbyexons[nearbyexons['start'] > three_ss].start.min()\n",
    "                junctions_three.append(str(geneid + ':chr'+ splice_jnc.chr + ':intron:'+ str(three_near_end_exon+1) +'-' + str(three_near_start_exon-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now give the name of the constitutiv exon or intron if it exists as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    five_con_exon_val = [x for x in five_con_exon if x not in [0,np.nan]]\n",
    "    five_con_intron_val = [x for x in five_con_intron if x not in [0,np.nan]]\n",
    "    if five_con_exon_val:\n",
    "        dict_five['constitutiveexon'] = ';'.join(five_con_exon_val)\n",
    "    elif 0 in five_con_exon:\n",
    "        dict_five['constitutiveexon'] = 0\n",
    "    else:\n",
    "        dict_five['constitutiveexon'] = 'NA'\n",
    "    if five_con_intron_val:\n",
    "        dict_five['constitutiveintron'] = ';'.join(five_con_intron_val)\n",
    "    elif 0 in five_con_intron:\n",
    "        dict_five['constitutiveintron'] = 0\n",
    "    else:\n",
    "        dict_five['constitutiveintron'] = 'NA'\n",
    "    three_con_exon_val = [x for x in three_con_exon if x not in [0,np.nan]]\n",
    "    three_con_intron_val = [x for x in three_con_intron if x not in [0,np.nan]]\n",
    "    if three_con_exon_val:\n",
    "        dict_three['constitutiveexon'] = ';'.join(three_con_exon_val)\n",
    "    elif 0 in three_con_exon:\n",
    "        dict_three['constitutiveexon'] = 0\n",
    "    else:\n",
    "        dict_three['constitutiveexon'] = 'NA'\n",
    "    if three_con_intron_val:\n",
    "        dict_three['constitutiveintron'] = ';'.join(three_con_intron_val)\n",
    "    elif 0 in three_con_intron:\n",
    "        dict_three['constitutiveintron'] = 0\n",
    "    else:\n",
    "        dict_three['constitutiveintron'] = 'NA'\n",
    "    junctions_five = list(set(junctions_five))\n",
    "    junctions_three = list(set(junctions_three))\n",
    "    dict_five['intron'] = five_intron\n",
    "    dict_three['intron'] = three_intron\n",
    "    if  dict_five['gene'] == 1:\n",
    "        dict_five['gene'] = ';'.join(list(set([itm.split(':',1)[0] for itm in junctions_five])))\n",
    "    if dict_three['gene'] == 1:\n",
    "        dict_three['gene'] = ';'.join(list(set([itm.split(':',1)[0] for itm in junctions_three])))\n",
    "    if dict_five['exon'] == 1:\n",
    "        dict_five['exon'] = ';'.join([(s.split(':')[0] +':'+ s.split(':')[-1]) for s in junctions_five if ':exon:' in s])\n",
    "    if dict_three['exon'] == 1:\n",
    "        dict_three['exon'] =';'.join([(s.split(':')[0] +':'+ s.split(':')[-1]) for s in junctions_three if ':exon:' in s])\n",
    "    if dict_five['intron'] == 1:\n",
    "        dict_five['intron'] = ';'.join([(s.split(':')[0] +':'+ s.split(':')[-1]) for s in junctions_five if ':intron:' in s])\n",
    "    if dict_three['intron'] == 1:\n",
    "        dict_three['intron'] = ';'.join([(s.split(':')[0] +':'+ s.split(':')[-1]) for s in junctions_three if ':intron:' in s])\n",
    "    #dict_five['intron'] = five_intron\n",
    "    #dict_three['intron'] = three_intron\n",
    "    dict_five['strandofgene'] = strand\n",
    "    dict_three['strandofgene'] = strand\n",
    "    #dict_five['otherregions'] = set(other_junctions_five)\n",
    "    #dict_three['otherregions'] = set(other_junctions_three)\n",
    "    return (dict_five, dict_three)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the closest other splice sites to each splice site of a splice junction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs and measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function takes in the splice junction, and a dataframe that contains all the splice junctions assocaited with the given strand and chromosome. The workflow of the function is as follows:\n",
    "1. Create new columns that measures the distance from the start and end site of each splice junction to the analyzed splice junction\n",
    "2. Find the closest one using max and min appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_splice_sites(jnc, splice_list):\n",
    "    gtf = splice_list\n",
    "    fb = jnc.first_base\n",
    "    lb = jnc.last_base\n",
    "    gtf['dist_start_fb'] = gtf['start'] - fb\n",
    "    gtf['dist_end_fb'] = gtf['end'] - fb\n",
    "    gtf['dist_start_lb'] = gtf['start'] - lb\n",
    "    gtf['dist_end_lb'] = gtf['end'] - lb\n",
    "    ss_fb = (gtf[\"dist_start_fb\"]== 0)\n",
    "    ss_lb = (gtf[\"dist_end_lb\"]== 0)\n",
    "    if len(gtf[ss_fb].index) == 0:\n",
    "        fb_annotation = 0\n",
    "    else:\n",
    "        fb_annotation = 1\n",
    "    fb_left_start = gtf[gtf['dist_start_fb'] <= 0].dist_start_fb.max()\n",
    "    fb_left_end = gtf[gtf['dist_end_fb'] <= 0].dist_end_fb.max()\n",
    "    fb_right_start = gtf[gtf['dist_start_fb'] >= 0].dist_start_fb.min()\n",
    "    fb_right_end = gtf[gtf['dist_end_fb'] >= 0].dist_end_fb.min()\n",
    "     if len(gtf[ss_lb].index) == 0:\n",
    "        lb_annotation = 0\n",
    "    else:\n",
    "        lb_annotation =1\n",
    "    lb_left_start = gtf[gtf['dist_start_lb'] <= 0].dist_start_lb.max()\n",
    "    lb_left_end = gtf[gtf['dist_end_lb'] <= 0].dist_end_lb.max()\n",
    "    lb_right_start = gtf[gtf['dist_start_lb'] >= 0].dist_start_lb.min()\n",
    "    lb_right_end = gtf[gtf['dist_end_lb'] >= 0].dist_end_lb.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the values in the appropriate columns based on the strand because upstream and downstream differ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    if jnc.strand == 1:\n",
    "        splicesite_dict = {\"5ss_to_upstream5ss\": fb_left_start, \"5ss_to_upstream3ss\": fb_left_end,\n",
    "                           \"5ss_to_downstream5ss\": fb_right_start,\"5ss_to_downstream3ss\": fb_right_end,\n",
    "                           \"3ss_to_upstream5ss\": lb_left_start, \"3ss_to_upstream3ss\": lb_left_end,\n",
    "                           \"3ss_to_downstream5ss\": lb_right_start,\"3ss_to_downstream3ss\": lb_right_end,\n",
    "                           \"5annotated\": fb_annotation, \"3annotated\":lb_annotation}\n",
    "    else:\n",
    "        splicesite_dict = {\"3ss_to_upstream5ss\": fb_right_end, \"3ss_to_upstream3ss\": fb_right_start,\n",
    "                           \"3ss_to_downstream5ss\": fb_left_end,\"3ss_to_downstream3ss\": fb_left_start,\n",
    "                           \"5ss_to_upstream5ss\": lb_right_end, \"5ss_to_upstream3ss\": lb_right_start,\n",
    "                           \"5ss_to_downstream5ss\": lb_left_end,\"5ss_to_downstream3ss\": lb_left_start,\n",
    "                           \"5annotated\": lb_annotation, \"3annotated\":fb_annotation}\n",
    "    return splicesite_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## phyloP function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs and initial setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function takes in a splice junction, a bigwig file, and the range of phyloP scores from each splice site to get individual values. \n",
    "\n",
    "First, we need to convert the bigwig file to a csv uisg bigWigToBedGraph\n",
    "\n",
    "The script uses a bash command to call bigWigToBedGraph. \n",
    "1. Create two dataframes of phyloP scores at each end of the splice junction\n",
    "2. Adjust the dataframe to go from 0-based indexing to 1-based indexing by adding 1 to the start site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phyloP_func(jnc, bigwig, rangeval):\n",
    "    bash_cmd = \"bigWigToBedGraph -chrom=chr%s -start=%i -end=%i %s data/phyloPscores.bedGraph\" % (jnc.chr, int(jnc.first_base - 100), int(jnc.last_base + 100), bigwig)\n",
    "    process = subprocess.Popen(bash_cmd.split(), stdout=subprocess.PIPE)\n",
    "    output, error = process.communicate()\n",
    "\n",
    "    phyloPinfo = pd.read_csv('data/phyloPscores.bedGraph', sep = '\\t', names = ['chr', 'start', 'end', 'score'])\n",
    "    phyloPinfo['start'] = phyloPinfo['start'] + 1\n",
    "    \n",
    "    \n",
    "    fb_extract = ((jnc.first_base + int(float(rangeval)/2)) >= phyloPinfo['start']) & ((jnc.first_base - int(float(rangeval)/2)) <= phyloPinfo['end'])\n",
    "    fb_phyloP = phyloPinfo[fb_extract]\n",
    "    fb_phyloP = fb_phyloP.sort_values(by = ['start'])\n",
    "    fb_phyloP[\"pos\"] = fb_phyloP[\"start\"] - jnc.first_base\n",
    "    fb_phyloP['length'] = fb_phyloP['end'] - fb_phyloP['start'] +1\n",
    "    lb_extract = ((jnc.last_base + int(float(rangeval)/2)) >= phyloPinfo['start']) & ((jnc.last_base - int(float(rangeval)/2)) <= phyloPinfo['end'])\n",
    "    lb_phyloP = phyloPinfo[lb_extract]\n",
    "    lb_phyloP = lb_phyloP.sort_values(by = ['start'])\n",
    "    lb_phyloP[\"pos\"] = lb_phyloP[\"start\"] - jnc.last_base\n",
    "    lb_phyloP['length'] = lb_phyloP['end'] - lb_phyloP['start'] +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look through each of the dataframes to get the phyloP scores. If it is empty, the average score and list of scores are NA. Otherwise, we look for missing values in the csv (maybe a phyloP score for a specific base is missing) and we replace those with NA in the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    if fb_phyloP.empty:\n",
    "        fb_score = 'NA'\n",
    "        fb_list_fin = ['NA']\n",
    "    else:\n",
    "        if ((jnc.first_base - int(float(rangeval)/2)) > fb_phyloP['start'].iloc[0]):\n",
    "            fb_phyloP['length'].iloc[0] = fb_phyloP['end'].iloc[0] - (jnc.first_base - int(float(rangeval)/2)) +1\n",
    "            fb_phyloP[\"pos\"].iloc[0] = int(float(rangeval)/2) *-1\n",
    "        elif ((jnc.first_base - int(float(rangeval)/2)) < fb_phyloP['start'].iloc[0]):\n",
    "            init_row = {'chr':fb_phyloP['chr'].iloc[0],'start':(jnc.first_base - int(float(rangeval)/2)), 'end':(fb_phyloP['start'].iloc[0]-1), 'pos':-1*int(float(rangeval)/2), 'length':                 fb_phyloP['start'].iloc[0]- (jnc.first_base - int(float(rangeval)/2))}\n",
    "            fb_phyloP = fb_phyloP.append(init_row, ignore_index = True)\n",
    "            fb_phyloP = fb_phyloP.sort_values(by = ['start'])\n",
    "        if ((jnc.first_base + int(float(rangeval)/2)) < fb_phyloP['end'].iloc[-1]):\n",
    "            fb_phyloP['length'].iloc[-1] = (jnc.first_base + int(float(rangeval)/2)) - fb_phyloP['start'].iloc[-1] +1\n",
    "            fb_phyloP[\"pos\"].iloc[-1] = int(float(rangeval)/2)\n",
    "        elif ((jnc.first_base + int(float(rangeval)/2)) > fb_phyloP['end'].iloc[-1]):\n",
    "            fin_row = {'chr':fb_phyloP['chr'].iloc[-1],'end':(jnc.first_base + int(float(rangeval)/2)), 'start':(fb_phyloP['end'].iloc[-1]+1), 'pos':int(float(rangeval)/2), 'length':(jnc.first_base +    int(float(rangeval)/2) - (fb_phyloP['end'].iloc[-1]+1) + 1)}\n",
    "            fb_phyloP = fb_phyloP.append(fin_row, ignore_index = True)\n",
    "            fb_phyloP = fb_phyloP.sort_values(by = ['start'])\n",
    "        fb_list_fin = [np.around(fb_phyloP['score'].iloc[0],3)] * fb_phyloP['length'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the average of the scores and the list of scores using numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        for i in range(1, len(fb_phyloP)):\n",
    "            if fb_phyloP['start'].iloc[i] - fb_phyloP['end'].iloc[i-1] == 1:\n",
    "                fb_list_fin.extend([np.around(fb_phyloP['score'].iloc[i],3)]*fb_phyloP['length'].iloc[i])\n",
    "            else:\n",
    "                fb_list_fin.extend(['NA']*(fb_phyloP['start'].iloc[i] - fb_phyloP['end'].iloc[i-1] - 1))\n",
    "                fb_list_fin.extend([np.around(fb_phyloP['score'].iloc[i],3)]*fb_phyloP['length'].iloc[i])\n",
    "        fb_score = np.average([val for val in fb_list_fin if (val not in [np.nan, 'NA']) and (not np.isnan(val))])\n",
    "        fb_score = np.around(fb_score,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the same process for the 3' splice site present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    if lb_phyloP.empty:\n",
    "        lb_score = 'NA'\n",
    "        lb_list_fin = ['NA']\n",
    "    else:\n",
    "        if ((jnc.last_base - int(float(rangeval)/2)) > lb_phyloP['start'].iloc[0]):\n",
    "            lb_phyloP['length'].iloc[0] = lb_phyloP['end'].iloc[0] - (jnc.last_base - int(float(rangeval)/2)) +1\n",
    "            lb_phyloP[\"pos\"].iloc[0] = int(float(rangeval)/2) *-1\n",
    "        elif ((jnc.last_base - int(float(rangeval)/2)) < lb_phyloP['start'].iloc[0]):\n",
    "            init_row = {'chr':lb_phyloP['chr'].iloc[0],'start':(jnc.last_base - int(float(rangeval)/2)), 'end':(lb_phyloP['start'].iloc[0]-1), 'pos':-1*int(float(rangeval)/2),'length':                   lb_phyloP['start'].iloc[0]- (jnc.last_base - int(float(rangeval)/2))}\n",
    "            lb_phyloP = lb_phyloP.append(init_row, ignore_index = True)\n",
    "            lb_phyloP = lb_phyloP.sort_values(by = ['start'])\n",
    "        if ((jnc.last_base + int(float(rangeval)/2)) < lb_phyloP['end'].iloc[-1]):\n",
    "            lb_phyloP['length'].iloc[-1] = (jnc.last_base + int(float(rangeval)/2)) - lb_phyloP['start'].iloc[-1] +1\n",
    "            lb_phyloP[\"pos\"].iloc[-1] = int(float(rangeval)/2)\n",
    "\n",
    "        elif ((jnc.last_base + int(float(rangeval)/2)) > lb_phyloP['end'].iloc[-1]):\n",
    "            fin_row = {'chr':lb_phyloP['chr'].iloc[-1],'end':(jnc.last_base + int(float(rangeval)/2)), 'start':(lb_phyloP['end'].iloc[-1]+1), 'pos':int(float(rangeval)/2), 'length':(jnc.last_base +      int(float(rangeval)/2) - (lb_phyloP['end'].iloc[-1]+1) + 1)}\n",
    "            lb_phyloP = lb_phyloP.append(fin_row, ignore_index = True)\n",
    "            lb_phyloP = lb_phyloP.sort_values(by = ['start'])\n",
    "        #lb_score = np.average(lb_phyloP.score, weights = lb_phyloP.length)\n",
    "        lb_list_fin = [np.around(lb_phyloP['score'].iloc[0],3)] * lb_phyloP['length'].iloc[0]\n",
    "        #lb_phyloP_fin = lb_phyloP[[ 'start','pos','score']]\n",
    "        #lb_phyloP_fin =  lb_phyloP_fin.set_index('start')\n",
    "        #lb_list = lb_phyloP_fin.values.tolist()\n",
    "        for i in range(1, len(lb_phyloP)):\n",
    "            if lb_phyloP['start'].iloc[i] - lb_phyloP['end'].iloc[i-1] == 1:\n",
    "                lb_list_fin.extend([np.around(lb_phyloP['score'].iloc[i],3)]*lb_phyloP['length'].iloc[i])\n",
    "            else:\n",
    "                lb_list_fin.extend(['NA']*(lb_phyloP['start'].iloc[i] - lb_phyloP['end'].iloc[i-1] - 1))\n",
    "                lb_list_fin.extend([np.around(lb_phyloP['score'].iloc[i],3)]*lb_phyloP['length'].iloc[i])\n",
    "        lb_score = np.average([val for val in lb_list_fin if (val not in [np.nan, 'NA']) and (not np.isnan(val))])\n",
    "        lb_score = np.around(lb_score,3)\n",
    "    return fb_score, fb_list_fin, lb_score, lb_list_fin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
