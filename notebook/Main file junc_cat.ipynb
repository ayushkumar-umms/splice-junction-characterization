{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main File (splicejunxchx.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following file is the main file that is used by this program in order to characterize the splice junctions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The required modules necessary for this python script to work is as follows:\n",
    "    python 3.6 or recent\n",
    "    pandas 0.24.0 or recent\n",
    "    bedtools\n",
    "    bigWigtoBedGraph\n",
    "    gtf2csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "usage: splicejunxchx [-h] [-seqs SEQUENCE_FILE] [-supp SUPPORT_FILES SUPPORT_FILES] [-phyloP PHYLOPSCORES PHYLOPSCORES] inputs inputs output_file\n",
    "\n",
    "positional arguments:\n",
    "  inputs                A GTF file and a STAR.out.tab file with information about splice junctions\n",
    "  output_file           Name of output file (.csv) to save final dataframe\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  -seqs SEQUENCE_FILE, --sequence_file SEQUENCE_FILE\n",
    "                        Fasta file for sequences of GTF file in order to get sequence info about splice junctions\n",
    "  -supp SUPPORT_FILES SUPPORT_FILES, --support_files SUPPORT_FILES SUPPORT_FILES\n",
    "                        Supply the splice junction file and constitutive exon file. This makes it quicker to run.\n",
    "  -phyloP PHYLOPSCORES PHYLOPSCORES, --phyloPscores PHYLOPSCORES PHYLOPSCORES\n",
    "                        phyloP score file (bigwig) and the number of nucleotides around each splice site "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "from sys import argv\n",
    "import subprocess\n",
    "import time\n",
    "from splicejunxchx.func_file import *\n",
    "from splicejunxchx.get_seq_splicesite import *\n",
    "from splicejunxchx.extract_splice_junctions import *\n",
    "from splicejunxchx.constitutive_exons import *\n",
    "from splicejunxchx.maxent import *\n",
    "import os\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Input Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up argparse to take in the necessary arguments from the command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-seqs SEQUENCE_FILE] [-supp SUPPORT_FILES SUPPORT_FILES] [-phyloP PHYLOPSCORES PHYLOPSCORES] [-maxEnt] inputs inputs output_file\n",
      "ipykernel_launcher.py: error: the following arguments are required: inputs, output_file\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushkumar/.pyenv/versions/3.8.2/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3339: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('inputs', nargs = 2, help = 'A GTF file (gtf.gz or .csv) and a STAR file (.out.tab) with information about splice junctions')\n",
    "parser.add_argument('-seqs','--sequence_file', help = 'Fasta file (.fa) for sequences of GTF file')\n",
    "parser.add_argument('-supp', '--support_files', nargs = 2, help = 'Supply the splice junction file (.csv) and constitutive exon file (.csv)')\n",
    "parser.add_argument('-phyloP', '--phyloPscores', nargs = 2, help = 'phyloP score file (.bw) and the number of nucleotides around each splice site (must be even number and less than 200)')\n",
    "parser.add_argument('-maxEnt', '--maxEntscore', action = 'store_true',help = 'If maxEnt, perl files and splicemodel directory are included in root then you can calculate the maxEnt score by adding  this optional flag')\n",
    "parser.add_argument('output_file', help = 'Name of output file (.csv) to save final dataframe')\n",
    "args = parser.parse_args()\n",
    "\n",
    "if (args.maxEntscore) & (args.sequence_file == None):\n",
    "    sys.exit(\"Cannot calculate maxEnt without proper sequence file (-seqs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using gtf2csv to convert the gtf.gz file into a csv file by using a bash command "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = str(args.inputs[0])\n",
    "if filename.endswith('.gtf.gz'): # if file is GTF then need to use gtf2csv in bash\n",
    "    gtfcsvfile = filename[:-7] + '.csv'\n",
    "    bash_cmd = \"gtf2csv -f %s\" % args.inputs[0]\n",
    "    process = subprocess.Popen(bash_cmd.split(), stdout=subprocess.PIPE)\n",
    "    output, error = process.communicate()\n",
    "elif filename.endswith('.gtf'): # if file is GTF then need to use gtf2csv in bash\n",
    "    gtfcsvfile = filename[:-4] + '.csv'\n",
    "    bash_cmd = \"gtf2csv -f %s\" % args.inputs[0]\n",
    "    process = subprocess.Popen(bash_cmd.split(), stdout=subprocess.PIPE)\n",
    "    output, error = process.communicate()\n",
    "elif filename.endswith('.csv'):# if file is already csv then just load into df\n",
    "    gtfcsvfile = filename\n",
    "else:\n",
    "    sys.exit(\"Need GTF file (.gtf.gz or .gtf) or CSV file for initial input\")\n",
    "gtf_df = pd.read_csv(gtfcsvfile, dtype = {'seqname': str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert a SJ.out.tab file into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnc_filename= str(args.inputs[1])\n",
    "if jnc_filename.endswith('out.tab'): # Create dataframe for SJ.out.tab\n",
    "    col_names = ['chr','first_base', 'last_base','strand','motif','annotation','uniq_reads', 'no_reads','overhang']\n",
    "    jnc_df =  pd.read_csv(args.inputs[1], names = col_names, sep = '\\t', dtype = {'chr': str})\n",
    "    jnc_df['unidentified_strand'] = [1 if x == 0 else 0 for x in jnc_df['strand']]\n",
    "    jncid_list = []\n",
    "    for id in range(1,len(jnc_df.index)+1):\n",
    "        jncid_list.append(\"JNC\" + str(id))\n",
    "    jnc_df['JNC_ID'] = jncid_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take splice junctions with unidentified strand and duplicate them to assume ones lies on the positive strand and the other on the negative strand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_strand_info = jnc_df[jnc_df['strand']==0]\n",
    "change_strand = set()\n",
    "for jncid in no_strand_info.itertuples():\n",
    "    change_strand.add((jncid.JNC_ID + '.1', jncid.chr, jncid.first_base, jncid.last_base, 1, jncid.motif, jncid.annotation, jncid.uniq_reads, jncid.no_reads, jncid.overhang,jncid.                   unidentified_strand))\n",
    "    change_strand.add((jncid.JNC_ID + '.2', jncid.chr, jncid.first_base, jncid.last_base, 2, jncid.motif, jncid.annotation, jncid.uniq_reads, jncid.no_reads, jncid.overhang, jncid.                  unidentified_strand))\n",
    "no_strand_df = pd.DataFrame(change_strand, columns = ['JNC_ID','chr','first_base', 'last_base','strand','motif','annotation','uniq_reads', 'no_reads','overhang','unidentified_strand'])\n",
    "jnc_df = jnc_df.append(no_strand_df, ignore_index=True)\n",
    "jnc_df.set_index(['JNC_ID'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all splice junctions based on the GTF file and all the constitutive exons (more information about the functions are found in the other files). These files are csv files that can be called by using the -supp tag if they are provided. Create the final dataframe which is a copy of the initial SJ.out.tab file that is provided. Sort the junctions based on chromosome and strand. This allows the code to optimize which junctions to analyze first by grouping the similarities together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (args.support_files != None):\n",
    "    all_spl_junc = pd.read_csv(args.support_files[0])\n",
    "    cons_exons = pd.read_csv(args.support_files[1])\n",
    "else:\n",
    "    all_spl_junc = all_spl_junctions(gtf_df)\n",
    "    cons_exons = constitutive_exons(gtf_df)\n",
    "final_df = jnc_df.iloc[:,np.r_[0:6,9]].copy()\n",
    "jnc_df = jnc_df.sort_values(by = ['chr','strand'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add columns to the final dataframe with information that we want to be included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_cols = [\"5'_in_gene\", \"5'_in_transcript\",\"5'_in_exon\",\"5'_in_constitutiveexon\",\"5'_in_intron\", \"5'_in_constitutiveintron\",\"5'_in_fiveprimeutr\",\"5'_in_threeprimeutr\",\"5'_in_CDS\", \"5'_in_startcodon\",\"5'_in_stopcodon\",  \"5'_in_specificregions\",  \"5'_in_otherregions\" , \"3'_in_gene\", \"3'_in_transcript\", \"3'_in_exon\",  \"3'_in_constitutiveexon\",\n",
    "                 \"3'_in_intron\",\"3'_in_constitutiveintron\",\"3'_in_fiveprimeutr\",\"3'_in_threeprimeutr\",\"3'_in_CDS\",\"3'_in_startcodon\",\"3'_in_stopcodon\",\"3'_in_specificregions\", \"3'_in_otherregions\",'alternative_splicing']\n",
    "splice_sites_nearby_cols = [\"5'annotated\", \"3'annotated\",\n",
    "                 \"first_base_to_upstream5'ss\", \"first_base_to_upstream3'ss\",\n",
    "                 \"first_base_to_downstream5'ss\",\"first_base_to_downstream3'ss\",\n",
    "                 \"last_base_to_upstream5'ss\", \"last_base_to_upstream3'ss\",\n",
    "                 \"last_base_to_downstream5'ss\",\"last_base_to_downstream3'ss\"]\n",
    "if args.sequence_file != None:\n",
    "    seq_info = [\"5'ss_sequence_51bp\", \"3'ss_sequence_51bp\", \"5'ss_sequence_2bp\",\"3'ss_sequence_2bp\",\"5'bases_maxEnt\",\"3'bases_maxEnt\", \"5'score_maxEnt\",\"3'score_maxEnt\"]\n",
    "if args.phyloPscores != None:\n",
    "    phyloP_cols = [\"5'phyloPscore\", \"3'phyloPscore\", \"5'phyloPlist\", \"3'phyloPlist\"]\n",
    "all_cols = add_cols +splice_sites_nearby_cols+seq_info+phyloP_cols\n",
    "for col in all_cols:\n",
    "    final_df[col] = np.nan\n",
    "    if col in [\"5'_in_specificregions\",\"3'_in_specificregions\", \"5'phyloPlist\", \"3'phyloPlist\"]:\n",
    "        final_df[col] = final_df[col].astype('object')\n",
    "    if col in seq_info:\n",
    "        final_df[col] = final_df[col].astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperate GTF file into positive strand and negative strand data to make it easier to search through. Also narrow down the columsn that have a 5'_ and 3'_ at the beginning in order to later use these columns to plug in the appropriat information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_strand = gtf_df['strand'] == '+'\n",
    "neg_strand = gtf_df['strand'] == '-'\n",
    "pos_gtf = gtf_df[pos_strand]\n",
    "neg_gtf = gtf_df[neg_strand]\n",
    "\n",
    "five_prime_str = \"5'_\"\n",
    "five_prime_cols = list(filter(lambda x: five_prime_str in x, final_df.columns))\n",
    "three_prime_str = \"3'_\"\n",
    "three_prime_cols = list(filter(lambda x: three_prime_str in x, final_df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the sequence information for the junctions that are provided. Create temp variables prev_jncrow_chr and prev_jncrow_strand to see if they match from the previous iteration to save time in creating a new extraction dataframe. The sequence_splice_site function is in get_seq_splicesites.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.process_time()\n",
    "if args.sequence_file != None:\n",
    "    seqs = sequence_splice_site(final_df[~final_df.strand.isin([0])], args.sequence_file) # Sequence file\n",
    "gtfseqnamelist = set(gtf_df['seqname'].tolist())\n",
    "prev_jncrow_chr = np.nan # temp variable to compare previous iteration\n",
    "prev_jncrow_strand = np.nan # temp variable to compare previous iteration\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating through each Junction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start iterating through each junction of the dataframe and begin using the following functions: extract_spljnc, closest_splice_sites, and phyloP_func (these functions are found in func_file.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for jncrow in jnc_df.itertuples(): # iterate through each splice junction\n",
    "    i+=1\n",
    "    if (jncrow.strand == 1) & (jncrow.chr in gtfseqnamelist): # Determine if its on the + or - strand\n",
    "        gtf_search = pos_gtf\n",
    "        if (jncrow.strand == prev_jncrow_strand) and (jncrow.chr == prev_jncrow_chr): # Determine if its on the same strand/chr as the previous iteration to save time\n",
    "            gtf_chr = prev_gtf_chr\n",
    "            gtf_exons = prev_gtf_exons\n",
    "            spl_junc_df = prev_spl_junc_df\n",
    "        else: # Create 3 dataframes: 1. gtf_chr corresponding chr in gtf file 2. corresponding exons from gtf file 3. splice junctions found on that chromosome and strand\n",
    "            gtf_chr = gtf_search[np.in1d(gtf_search['seqname'].values, [jncrow.chr])]\n",
    "            gtf_exons = gtf_chr[np.in1d(gtf_chr['feature'].values, ['exon'])]\n",
    "            spl_junc_df = all_spl_junc[np.in1d(all_spl_junc['seqname'].values,[jncrow.chr])]\n",
    "            spl_junc_df = spl_junc_df[np.in1d(spl_junc_df['strand'].values, ['+'])]\n",
    "        five_dict, three_dict = extract_spljnc(jncrow, gtf_chr, '+', cons_exons, gtf_exons) # outputs dictionary of 5' and 3' info to add to final dataframe\n",
    "        #final_df.at[jncrow.Index,'alternative_splicing'] = alt_vs_cons_splicing(jncrow, gtf_chr)\n",
    "        ss_nearby = closest_splice_sites(jncrow, spl_junc_df)\n",
    "\n",
    "    elif (jncrow.strand == 2) & (jncrow.chr in gtfseqnamelist):\n",
    "        gtf_search = neg_gtf\n",
    "        if (jncrow.strand == prev_jncrow_strand) and (jncrow.chr == prev_jncrow_chr):\n",
    "            gtf_chr = prev_gtf_chr\n",
    "            gtf_exons = prev_gtf_exons\n",
    "            spl_junc_df = prev_spl_junc_df\n",
    "        else:\n",
    "            gtf_chr = gtf_search[np.in1d(gtf_search['seqname'].values, [jncrow.chr])]\n",
    "            gtf_exons = gtf_chr[np.in1d(gtf_chr['feature'].values, ['exon'])]\n",
    "            spl_junc_df = all_spl_junc[np.in1d(all_spl_junc['seqname'].values,[jncrow.chr])]\n",
    "            spl_junc_df = spl_junc_df[np.in1d(spl_junc_df['strand'].values, ['-'])]\n",
    "        five_dict, three_dict = extract_spljnc(jncrow, gtf_chr, '-', cons_exons, gtf_exons) # outputs dictionary of 5' and 3' info to add to final dataframe\n",
    "        #final_df.at[jncrow.Index,'alternative_splicing'] = alt_vs_cons_splicing(jncrow, gtf_chr)\n",
    "        ss_nearby = closest_splice_sites(jncrow, spl_junc_df)\n",
    "\n",
    "    else: # Do not analyze strands that have a unidentified strand\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    # If phyloPscore is requested, then using the phyloP_func to get the average score and the arrays for specific values\n",
    "    if args.phyloPscores != None:\n",
    "        fb_phyloPval, fb_extract_phyloP, lb_phyloPval, lb_extract_phyloP = phyloP_func(jncrow,args.phyloPscores[0], args.phyloPscores[1])\n",
    "        if jncrow.strand == 1:\n",
    "            final_df.at[jncrow.Index, \"5'phyloPscore\"] = fb_phyloPval\n",
    "            final_df.at[jncrow.Index, \"5'phyloPlist\"] = fb_extract_phyloP\n",
    "            final_df.at[jncrow.Index, \"3'phyloPscore\"] = lb_phyloPval\n",
    "            final_df.at[jncrow.Index, \"3'phyloPlist\"] = lb_extract_phyloP\n",
    "        elif jncrow.strand == 2:\n",
    "            final_df.at[jncrow.Index, \"3'phyloPscore\"] = fb_phyloPval\n",
    "            final_df.at[jncrow.Index, \"3'phyloPlist\"] = fb_extract_phyloP\n",
    "            final_df.at[jncrow.Index, \"5'phyloPscore\"] = lb_phyloPval\n",
    "            final_df.at[jncrow.Index, \"5'phyloPlist\"] = lb_extract_phyloP\n",
    "        else:\n",
    "            continue\n",
    "            #final_df.at[jncrow.Index, \"3'phyloPscore\"] = lb_phyloPval\n",
    "            #final_df.at[jncrow.Index, \"5'phyloPscore\"] = fb_phyloPval\n",
    "\n",
    "    for val in five_prime_cols: # Add all 5' info into the appropriate columns\n",
    "        if val.split('_')[-1] not in five_dict:\n",
    "            final_df.at[jncrow.Index,val] = 0\n",
    "        else:\n",
    "            final_df.at[jncrow.Index,val] = five_dict[val.split('_')[-1]]\n",
    "    for val in three_prime_cols: # Add all 3' info into the appropriate columns\n",
    "        if val.split('_')[-1] not in three_dict:\n",
    "            final_df.at[jncrow.Index,val] = 0\n",
    "        else:\n",
    "            final_df.at[jncrow.Index,val] = three_dict[val.split('_')[-1]]\n",
    "    for val in splice_sites_nearby_cols:\n",
    "        final_df.at[jncrow.Index, val] = ss_nearby[val]\n",
    "    \n",
    "    \n",
    "    # Creating temp variables of extracted dataframes to save time on re-extracting the information\n",
    "    prev_jncrow_chr = jncrow.chr\n",
    "    prev_jncrow_strand = jncrow.strand\n",
    "    prev_gtf_chr = gtf_chr\n",
    "    prev_gtf_exons= gtf_exons\n",
    "    prev_spl_junc_df = spl_junc_df\n",
    "    if i % 10000 == 0:\n",
    "        print(time.process_time() - start, '%i of %i' % (i,len(jnc_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling Final Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If sequence information is provided and maxEnt flag is present, then the sequence extraction is setup as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.sequence_file != None:\n",
    "    seqs_site3 = seqs[seqs['site'].isin(['3',3])].set_index('JNC_ID') # get sequences for 3' end\n",
    "    seqs_site5 = seqs[seqs['site'].isin(['5',5])].set_index('JNC_ID') # get sequences for 5' end\n",
    "    seqs_site3 =  seqs_site3.reindex(index=seqs_site3.index.to_series().str.slice(start=3).astype(float).sort_values().index) # order sequences based on JNC_ID index values\n",
    "    seqs_site5 = seqs_site5.reindex(index=seqs_site5.index.to_series().str.slice(start=3).astype(float).sort_values().index)\n",
    "    final_df = final_df.reindex(index=final_df.index.to_series().str.slice(start=3).astype(float).sort_values().index) # order final_df based on JNC_ID index values\n",
    "    final_df.loc[final_df.index.isin(seqs_site3.index.tolist()),\"3'ss_sequence_51bp\"] = seqs_site3['seq'].tolist() # Attach list of sequences to series\n",
    "    final_df.loc[final_df.index.isin(seqs_site5.index.tolist()),\"5'ss_sequence_51bp\"] = seqs_site5['seq'].tolist() # Attach list of sequences to series\n",
    "\n",
    "    # Extract sequences needed for maxEnt and the 2bp at the 5' and 3' end\n",
    "\n",
    "    final_df.loc[~final_df[\"5'ss_sequence_51bp\"].isin([np.nan,'nan']), \"5'ss_sequence_2bp\"] = final_df.loc[~final_df[\"5'ss_sequence_51bp\"].isin([np.nan, 'nan'])][\"5'ss_sequence_51bp\"].str.          slice(start = 25, stop = 27)\n",
    "    final_df.loc[~final_df[\"5'ss_sequence_51bp\"].isin([np.nan, 'nan']), \"5'bases_maxEnt\"] = final_df.loc[~final_df[\"5'ss_sequence_51bp\"].isin([np.nan, 'nan'])][\"5'ss_sequence_51bp\"].str.            slice(start = 22, stop =31)\n",
    "    final_df.loc[~final_df[\"3'ss_sequence_51bp\"].isin([np.nan, 'nan']), \"3'ss_sequence_2bp\"] = final_df.loc[~final_df[\"3'ss_sequence_51bp\"].isin([np.nan, 'nan'])][\"3'ss_sequence_51bp\"].str.         slice(start = 24, stop =  26)\n",
    "    final_df.loc[~final_df[\"3'ss_sequence_51bp\"].isin([np.nan, 'nan']), \"3'bases_maxEnt\"] = final_df.loc[~final_df[\"3'ss_sequence_51bp\"].isin([np.nan, 'nan'])][\"3'ss_sequence_51bp\"].str.            slice(start = 6, stop = 29)\n",
    "    if args.maxEntscore:\n",
    "        max_ent_seq_info = final_df.loc[~final_df[\"5'bases_maxEnt\"].isin([np.nan,'', 'nan']) & ~final_df[\"3'bases_maxEnt\"].isin([np.nan,'', 'nan'])]\n",
    "        max_ent_seq_info.to_csv('data/max_ent_seq.csv', header = True, index = True)\n",
    "        max_ent_series_5, max_ent_series_3 = calculate_maxEnt(max_ent_seq_info)\n",
    "        final_df.loc[~final_df[\"5'bases_maxEnt\"].isin([np.nan,'','nan']) & ~final_df[\"3'bases_maxEnt\"].isin([np.nan,'','nan']), \"5'score_maxEnt\"] = max_ent_series_5[\"maxEnt_5\"].tolist()\n",
    "        final_df.loc[~final_df[\"5'bases_maxEnt\"].isin([np.nan,'','nan']) & ~final_df[\"3'bases_maxEnt\"].isin([np.nan,'','nan']), \"3'score_maxEnt\"] = max_ent_series_3[\"maxEnt_3\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final csv file to publish:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output csv file\n",
    "output_path = str(args.output_file).rsplit('/',1)\n",
    "if os.path.exists(output_path[0]):\n",
    "    output = final_df.to_csv(args.output_file,header = True)\n",
    "else:\n",
    "    os.makedirs(output_path[0])\n",
    "    output = final_df.to_csv(args.output_file,header = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
